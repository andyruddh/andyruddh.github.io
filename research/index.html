<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> research | Aniruddh Puranic </title> <meta name="author" content="Aniruddh G. Puranic"> <meta name="description" content="overview of my research and selected projects"> <meta name="keywords" content="aniruddh-puranic, aniruddh, puranic, robotics, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.ico?eeb3dfe561cf79aa7e7b5255f48e1dbc"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://andyruddh.github.io/research/"> <script src="/assets/js/theme.js?bd888c560287cd675855c7662a167c4a"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Aniruddh Puranic </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">music + tennis </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">research</h1> <p class="post-description">overview of my research and selected projects</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_overview-480.webp 480w,/assets/img/research_overview-800.webp 800w,/assets/img/research_overview-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/research_overview.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="My research overview" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An overview of my research for developing safe and explainable robots that interact with humans. </div> <h4 id="neuro-symbolic-ai"><strong>NEURO-SYMBOLIC AI</strong></h4> <hr> <div class="row"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ral_iros2021-480.webp 480w,/assets/img/publication_preview/ral_iros2021-800.webp 800w,/assets/img/publication_preview/ral_iros2021-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/ral_iros2021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-8 mt-3 mb-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/al_stl-480.webp 480w,/assets/img/publication_preview/al_stl-800.webp 800w,/assets/img/publication_preview/al_stl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/al_stl.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Reward modeling and prediction via Gaussian processes and deep neural networks. Right: Extracted specification-consistent behaviors in simulations, including Nvidia Isaac. </div> <h5 id="learning-reward-functions-and-control-policies-that-satisfy-temporal-logic-specifications"><em>Learning reward functions and control policies that satisfy temporal-logic specifications</em></h5> <p>Designing dense or “informative” reward functions for Reinforcement Learning (RL) is a highly non-trivial task. Errors in reward design can lead to unsafe and undesirable learned control behaviors. This work introduces a neurosymbolic learning-from-demonstrations (LfD) framework that uses high-level tasks expressed in Signal Temporal Logic (STL), and user demonstrations to extract reward functions and control policies via reinforcement learning. The LfD-STL framework enables an agent to learn non-Markovian/temporal rewards and overcome critical issues (safety and performance) with inverse reinforcement learning methods. The initial development of the framework was applied to discrete and deterministic environments, and was later generalized to continuous spaces and stochastic environments via Gaussian Processes and neural-network modeling.</p> <h5 id="learning-to-improveextrapolate-beyond-demonstrator-performance"><em>Learning to improve/extrapolate beyond demonstrator performance</em></h5> <p>Generally, a machine learning model’s performance is determined by the quality and amount of data it is trained on. Thus, noisy data and limited human demonstrations, which is widely observed in robotic settings, poses a challenge to learn optimal behaviors. This work on neuro-symbolic apprenticeship learning implements temporal logic-guided reinforcement learning from demonstrations to automatically improve robot safety and performance via self-monitoring and adaptation. The capabilities of the framework are exhibited on a variety of mobile navigation, fixed-base manipulation and mobile-manipulation tasks using the <strong>Nvidia Isaac</strong> simulator. This paper is published in the proceedings of IROS 2024. Additional details can be found on the <a href="https://aniruddh-puranic.info/assets/pdf/alstl_supp.pdf" rel="external nofollow noopener" target="_blank">supplemental document</a>.</p> <h6 id="references"><strong>References</strong></h6> <ul> <li><span id="PDN_ALSTL">Puranic, A. G., Deshmukh, J. V., &amp; Nikolaidis, S. (2024). Signal Temporal Logic-Guided Apprenticeship Learning. <i>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, 11147–11154. https://doi.org/10.1109/IROS58592.2024.10801924</span></li> <li><span id="PDN21">Puranic, A. G., Deshmukh, J. V., &amp; Nikolaidis, S. (2021). Learning From Demonstrations Using Signal Temporal Logic in Stochastic and Continuous Domains. <i>IEEE Robotics and Automation Letters (RA-L). Presented at IROS</i>, <i>6</i>(4), 6250–6257. https://doi.org/10.1109/LRA.2021.3092676</span></li> <li><span id="PDN20">Puranic, A., Deshmukh, J., &amp; Nikolaidis, S. (2021). Learning from Demonstrations using Signal Temporal Logic. <i>Proceedings of the 2020 Conference on Robot Learning (CoRL)</i>, <i>155</i>, 2228–2242. https://proceedings.mlr.press/v155/puranic21a.html</span></li> </ul> <h4 id="interpretableexplainable-ai-xai"><strong>INTERPRETABLE/EXPLAINABLE AI (xAI)</strong></h4> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/peglearn_graphs-480.webp 480w,/assets/img/publication_preview/peglearn_graphs-800.webp 800w,/assets/img/publication_preview/peglearn_graphs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/peglearn_graphs.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/peglearn_rewards-480.webp 480w,/assets/img/publication_preview/peglearn_rewards-800.webp 800w,/assets/img/publication_preview/peglearn_rewards-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/peglearn_rewards.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mining_stl-480.webp 480w,/assets/img/publication_preview/mining_stl-800.webp 800w,/assets/img/publication_preview/mining_stl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/mining_stl.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Generating graphs that explain demonstrator performance and formal specification conflicts. Center: Neural reward modeling from inferred graphs. Right: Mining formal specifications from time-series data. </div> <h5 id="generating-explainable-temporal-logic-graphs-from-human-data"><em>Generating explainable temporal logic graphs from human data</em></h5> <p>Understanding and evaluating the human demonstrations and learned robot behaviors plays a critical role in optimizing the control policies for robots, without which, a robot may infer incorrect reward functions that lead to undesirable or unsafe control policies. The prior LfD-STL required the demonstrators to explicitly specify their preferences by ranking the STL specifications. The ranked specifications were represented by a directed acyclic graph (DAG) to capture the preferences and dependencies. To relax this manual burden, we automatically infer the specification DAG from demonstrations via our novel Performance Graph Learning (PeGLearn). PeGLearn facilitates explainability for AI-based systems via a user study on CARLA, a simulated driving environment. We also integrate human feedback (annotations) in a robot-assisted surgical domain to model behaviors of surgeons according to their expertise. Additional details can be found on the <a href="https://aniruddh-puranic.info/assets/pdf/peglearn_supp.pdf" rel="external nofollow noopener" target="_blank">supplemental document</a>.</p> <h5 id="learning-mining-specifications-from-temporal-data"><em>Learning (mining) specifications from temporal data</em></h5> <p>Autonomous cyber-physical systems such as self-driving cars, unmanned aerial vehicles, general purpose robots, and medical devices can often be modeled as a system consisting of heterogeneous components. Understanding the high-level behavior of such components, especially equipped with deep learning, at an abstract, behavioral level is thus a significant challenge. Our work seeks to answer: <em>Given a requirement on the system output behaviors, what are the assumptions on the model environment, i.e., inputs to the model, that guarantee that the corresponding output traces satisfy the output requirement?</em> We develop techniques involving decision-tree classifiers, counterexample-guided learning, optimization, enumeration and parameter mining to extract STL specifications that explain system behaviors.</p> <h6 id="references-1"><strong>References</strong></h6> <ul> <li><span id="PDN23">Puranic, A. G., Deshmukh, J. V., &amp; Nikolaidis, S. (2023). Learning Performance Graphs From Demonstrations via Task-Based Evaluations. <i>IEEE Robotics and Automation Letters (RA-L). Oral Presentation at ICRA</i>, <i>8</i>(1), 336–343. https://doi.org/10.1109/LRA.2022.3226072</span></li> <li><span id="HSCC20">Mohammadinejad, S., Deshmukh, J. V., Puranic, A. G., Vazquez-Chanlatte, M., &amp; Donzé, A. (2020). Interpretable Classification of Time-Series Data Using Efficient Enumerative Techniques. <i>Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control (HSCC)</i>. https://doi.org/10.1145/3365365.3382218</span></li> <li><span id="ICCPS20">Mohammadinejad, S., Deshmukh, J. V., &amp; Puranic, A. G. (2020). Mining Environment Assumptions for Cyber-Physical System Models. <i>2020 ACM/IEEE 11th International Conference on Cyber-Physical Systems (ICCPS)</i>, 87–97. https://doi.org/10.1109/ICCPS48487.2020.00016</span></li> </ul> <h4 id="computer-vision"><strong>COMPUTER VISION</strong></h4> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tqtl-480.webp 480w,/assets/img/publication_preview/tqtl-800.webp 800w,/assets/img/publication_preview/tqtl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/tqtl.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/keck_2019-480.webp 480w,/assets/img/publication_preview/keck_2019-800.webp 800w,/assets/img/publication_preview/keck_2019-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/publication_preview/keck_2019.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reward modeling" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: Generating graphs that explain demonstrator performance and formal specification conflicts. Center: Neural reward modeling from inferred graphs. Right: Mining formal specifications from time-series data. </div> <h5 id="evaluating-the-quality-of-vision-based-perception-algorithms"><em>Evaluating the quality of vision-based perception algorithms</em></h5> <p>Computer vision is one of the major perception components of a cyber-physical system with numerous applications in autonomous vehicles, industrial/factory robotics, medical devices, etc. Checking the correctness and ensuring robustness of perception algorithms such as those based on deep convolutional neural networks is a major challenge. Conventionally, perception algorithms are tested by comparing their performance to ground truth labels, that require a laborious annotation process. We propose the use of Timed Quality Temporal Logic (TQTL) as a formal language to express desirable spatio-temporal properties of a perception algorithm processing a video, offering an alternative metric that can provide useful information, even in the absence of ground truth labels.</p> <h5 id="vision-based-metric-for-evaluating-surgeons-performance"><em>Vision-based metric for evaluating surgeon’s performance</em></h5> <p>Due to the lack of instrument force feedback during robot-assisted surgery, tissue-handling technique is an important aspect of surgical performance to assess. We develop a vision-based machine learning algorithm for object detection and distance prediction to measure needle entry point deviation in tissue during robotic suturing as a proxy for tissue trauma.</p> <h6 id="references-2"><strong>References</strong></h6> <ul> <li><span id="DATE19">Balakrishnan, A., Puranic, A. G., Qin, X., Dokhanchi, A., Deshmukh, J. V., Ben Amor, H., &amp; Fainekos, G. (2019). Specifying and Evaluating Quality Metrics for Vision-based Perception Systems. <i>2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</i>, 1433–1438. https://doi.org/10.23919/DATE.2019.8715114</span></li> <li><span id="KECK19">Puranic, A., Chen, J., Nguyen, J., Deshmukh, J., &amp; Hung, A. (2019). MP35-04 Automated Evaluation of Instrument Force Sensitivity During Robotic Suturing Utilizing Vision-Based Machine Learning. <i>Journal of Urology</i>, <i>201</i>(Supplement 4), e505–e506. https://doi.org/10.1097/01.JU.0000555994.79498.94</span></li> </ul> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Aniruddh G. Puranic. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?3577194613afa04501eb52f8f4164de9" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>